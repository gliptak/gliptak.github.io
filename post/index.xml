<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>https://gliptak.github.io/post/</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016-2021 Gábor Lipták</copyright>
    <lastBuildDate>Sun, 27 Jun 2021 01:47:41 -0400</lastBuildDate><atom:link href="https://gliptak.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Packaging AWS Lambda functions with large dependencies as containers</title>
      <link>https://gliptak.github.io/post/aws_lambda_container/</link>
      <pubDate>Sun, 27 Jun 2021 01:47:41 -0400</pubDate>
      
      <guid>https://gliptak.github.io/post/aws_lambda_container/</guid>
      <description>While the actual application code for many lambda functions might be small, in many (data science) usecases libraries used and to be packaged together push over the deployment package limit of 50 MB compressed. Container packaging to the rescue.
A sample Dockerfile installing SageMaker Python SDK
ARG WORK_DIR=&amp;quot;/home/app/&amp;quot; FROM public.ecr.aws/lambda/python:3.8 ARG WORK_DIR RUN pip install wheel sagemaker COPY main.py . WORKDIR ${WORK_DIR} CMD [&amp;quot;main.handler&amp;quot;] and corresponding main.py
import sagemaker import sys def handler(event, context): print(sys.</description>
    </item>
    
    <item>
      <title>Running Jupyterhub Docker</title>
      <link>https://gliptak.github.io/post/jupyterhub_docker/</link>
      <pubDate>Sat, 19 Dec 2020 17:07:05 -0400</pubDate>
      
      <guid>https://gliptak.github.io/post/jupyterhub_docker/</guid>
      <description>Jupyterhub doesn&amp;rsquo;t seem to have complete instructions available on simply running an instance under Docker with a number of issues, Stackoverflow questions asking for clarifications. These are my quick notes:
As the Docker image name indicates, this is singleuser instance preconfigured for user jovyan. Run below command in directory with your notebooks
docker run --rm -p 8000:8000 -d -v `pwd`:/home/jovyan/work --name jupyterhub jupyterhub/singleuser jupyterhub There doesn&amp;rsquo;t seem to be (default) password set for jovyan user, so at this point you cannot login into the GUI.</description>
    </item>
    
    <item>
      <title>Splunk custom input with session</title>
      <link>https://gliptak.github.io/post/splunk_custom_input_session/</link>
      <pubDate>Sat, 31 Oct 2020 11:17:30 -0400</pubDate>
      
      <guid>https://gliptak.github.io/post/splunk_custom_input_session/</guid>
      <description>Splunk can be extended with custom inputs written in Python. In order to connect to Splunk services, code has to be configured and use a session token. Here is a basic setup emitting records with kvstore names
#!/usr/bin/env python import splunklib.client as client import sys import datetime as dt def generate(session_key): service = client.connect(token = session_key) for collection in service.kvstore: ts = dt.datetime.now(tz=dt.timezone.utc).isoformat() print(f&#39;{ts}, collection=&amp;quot;{collection.name}&amp;quot;&#39;) if __name__ == &amp;quot;__main__&amp;quot;: session_key = sys.</description>
    </item>
    
    <item>
      <title>AWS Account Cleanup</title>
      <link>https://gliptak.github.io/post/aws_account_cleanup/</link>
      <pubDate>Sat, 31 Oct 2020 10:17:30 -0400</pubDate>
      
      <guid>https://gliptak.github.io/post/aws_account_cleanup/</guid>
      <description>Many AWS customers skip initial planning steps and don&amp;rsquo;t establish infrastructure-as-code scripting practices initially necessitating querying and cleaning up resources to avoid billing charges.
AWS tooling  AWS Cost Explorer AWS Billing Console AWS Tag Editor select all regions, all resource types and uncheck tags  The open source community developed a number of tools
List all AWS resources  aws_list_all command line awsls AWSRetriever desktop aws-inventory command line and GUI  Filtered delete  aws-nuke extensive filtering cloud-nuke filtering by region, age, resourcetype is available  </description>
    </item>
    
    <item>
      <title>AWS Lambda Lookup AccountId and Region</title>
      <link>https://gliptak.github.io/post/aws_lambda_accountregion/</link>
      <pubDate>Fri, 02 Oct 2020 18:32:25 -0400</pubDate>
      
      <guid>https://gliptak.github.io/post/aws_lambda_accountregion/</guid>
      <description>For a number of uses including generating IAM roles and various policies, lambda code might require access to current account id and region. Here is a code snippet on how to acquire those values:
import json import logging import boto3 logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): logger.info(json.dumps(event)) account_id = boto3.client(&#39;sts&#39;).get_caller_identity().get(&#39;Account&#39;) # use a client which is region based logs_client = boto3.client(&#39;logs&#39;) region_name = logs_client.meta.region_name logger.info(f&amp;quot;{account_id}:{region_name}&amp;quot;) </description>
    </item>
    
    <item>
      <title>AWS EC2 Instance Connect</title>
      <link>https://gliptak.github.io/post/aws_ec2instanceconnect/</link>
      <pubDate>Fri, 29 May 2020 11:17:30 -0400</pubDate>
      
      <guid>https://gliptak.github.io/post/aws_ec2instanceconnect/</guid>
      <description>EC2 Instance Connect is somewhat overlooked functionality improving security of EC2 logins. During configuration, the default instance user is assigned a public key so it&amp;rsquo;s private pair can be used to connect to the instance. The private key tends to be shared within support teams and logins can no longer be attributed to an individual. EC2 instance Connect runs and logs connect commands as the individual user and obeys user&amp;rsquo;s permission.</description>
    </item>
    
    <item>
      <title>AWS S3 Signed Upload URL Basics</title>
      <link>https://gliptak.github.io/post/aws_s3_signed_url/</link>
      <pubDate>Thu, 19 Mar 2020 17:32:25 -0400</pubDate>
      
      <guid>https://gliptak.github.io/post/aws_s3_signed_url/</guid>
      <description>While I found a number of examples for generating signed upload S3 URLs, there didn&amp;rsquo;t seem to be examples with the basics.
After substituting the name for your bucket, file name and expiry desired, run below code to generate the URL:
import boto3 if __name__ == &amp;quot;__main__&amp;quot;: s3_client = boto3.client(&#39;s3&#39;) response = s3_client.generate_presigned_url( ClientMethod=&#39;put_object&#39;, Params={&amp;quot;Bucket&amp;quot;: &amp;quot;mybucket&amp;quot;, &amp;quot;Key&amp;quot;: &amp;quot;file.pdf&amp;quot;}, ExpiresIn=48*60*60, HttpMethod=&amp;quot;PUT&amp;quot;) print(response) To upload from command line run below (substituting URL from previous section):</description>
    </item>
    
    <item>
      <title>AWS Cloudformation Referencing AMIs Using SSM Parameter Store</title>
      <link>https://gliptak.github.io/post/aws_ssm_ami/</link>
      <pubDate>Wed, 11 Sep 2019 21:32:25 -0400</pubDate>
      
      <guid>https://gliptak.github.io/post/aws_ssm_ami/</guid>
      <description>When AWS infrastructure configured in &amp;ldquo;traditional&amp;rdquo; compute/storage/network style, identifying, referencing and patching AMIs in all regions in use is crucial. Cloudformation has a way to redirect AMI references through SSM Parameter Store.
This represents a tradeoff, as recreating the Cloudformation stack might pickup the next (patched) AMI hence it is no longer immutable. But resulting state is similar to externally patched Linux/Windows images which also cannot be recreated by simply redeploying Cloudformation.</description>
    </item>
    
    <item>
      <title>AWS Organizations CLI</title>
      <link>https://gliptak.github.io/post/aws_organization_cli/</link>
      <pubDate>Sat, 07 Sep 2019 21:17:30 -0400</pubDate>
      
      <guid>https://gliptak.github.io/post/aws_organization_cli/</guid>
      <description>Many AWS customers take advantage of AWS Organizations to organize and secure their workloads. In many cases, users login into their master account and configure permissions allowing to switch to member accounts in the Console. The same permissions can be used for AWS CLI.
In this example below, a single AWS Access Key has to be generated (and rotated) in the master account and it can be used to switch to test/2222222 and production/3333333 accounts using the CrossAccountAccessRole already configured for switching in the Console.</description>
    </item>
    
    <item>
      <title>Login into a CA SSO/Siteminder protected site with Python Requests</title>
      <link>https://gliptak.github.io/post/python-requests-siteminder/</link>
      <pubDate>Fri, 02 Jun 2017 19:07:05 -0400</pubDate>
      
      <guid>https://gliptak.github.io/post/python-requests-siteminder/</guid>
      <description>While this below code is simple, it uses two important approaches:
 utilizes a Requests Session to keep Siteminder login cookies/headers it has a two step load, allowing to fill out the Siteminder form  import requests if __name__ == &amp;#34;__main__&amp;#34;: mysite = &amp;#39;http://mysite/&amp;#39; credentials = {&amp;#39;USER&amp;#39;: &amp;#39;myuser&amp;#39;, &amp;#39;PASSWORD&amp;#39;: &amp;#39;mypassword&amp;#39;} s = requests.session() # use Session to keep cookies around page = s.get(mysite) s.post(page.url, data=credentials) # page.url is the Siteminder login screen page = s.</description>
    </item>
    
    <item>
      <title>AWS SimpleDB Boto3 Example</title>
      <link>https://gliptak.github.io/post/simpledb-example/</link>
      <pubDate>Wed, 15 Feb 2017 17:51:55 -0500</pubDate>
      
      <guid>https://gliptak.github.io/post/simpledb-example/</guid>
      <description>When looking into AWS SimpleDB, a quick search didn&amp;rsquo;t return any short Python Boto3 examples. So I decided to post one. (As with any services you to subscribe to, running this code below might cost you money &amp;hellip;)
from __future__ import print_function import boto3 def quote(string): return string.replace(&amp;#34;&amp;#39;&amp;#34;, &amp;#34;&amp;#39;&amp;#39;&amp;#34;).replace(&amp;#39;&amp;#34;&amp;#39;, &amp;#39;&amp;#34;&amp;#34;&amp;#39;).replace(&amp;#39;`&amp;#39;, &amp;#39;``&amp;#39;) def put_attributes(sdb, domain, id, color): response = sdb.put_attributes( DomainName=domain, ItemName=id, Attributes=[ { &amp;#39;Name&amp;#39;: &amp;#39;color&amp;#39;, &amp;#39;Value&amp;#39;: color, &amp;#39;Replace&amp;#39;: True }, ], ) print(response) if __name__ == &amp;#34;__main__&amp;#34;: domain = &amp;#34;TEST_DOMAIN&amp;#34; sdb = boto3.</description>
    </item>
    
  </channel>
</rss>
